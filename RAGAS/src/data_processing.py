"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ RAG —Å–∏—Å—Ç–µ–º–µ.
–í–∫–ª—é—á–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã.
"""

import os
import json
import logging
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
import pandas as pd
from datasets import Dataset, load_dataset
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_community.document_loaders import (
    TextLoader, 
    PyPDFLoader, 
    DirectoryLoader,
    UnstructuredFileLoader
)
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma, FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.retrievers import BaseRetriever
# MMRRetriever –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–µ—Ä—Å–∏—è—Ö
# from langchain_community.retrievers.mmr import MMRRetriever
import chromadb
from chromadb.config import Settings
from tqdm import tqdm

logger = logging.getLogger(__name__)


class DataProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ RAG —Å–∏—Å—Ç–µ–º–µ."""
    
    def __init__(self, config: Dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ YAML —Ñ–∞–π–ª–∞
        """
        self.config = config
        self.text_splitter = None
        self.vector_store = None
        self.retriever = None
        
    def _create_text_splitter(self) -> Union[RecursiveCharacterTextSplitter, CharacterTextSplitter]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏.
        
        Returns:
            Text splitter –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        """
        splitter_type = self.config['data'].get('text_splitter', 'recursive')
        chunk_size = self.config['data']['chunk_size']
        chunk_overlap = self.config['data']['chunk_overlap']
        
        if splitter_type == 'recursive':
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
        else:
            splitter = CharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separator="\n"
            )
        
        self.text_splitter = splitter
        logger.info(f"–°–æ–∑–¥–∞–Ω {splitter_type} —Å–ø–ª–∏—Ç—Ç–µ—Ä: chunk_size={chunk_size}, overlap={chunk_overlap}")
        return splitter
    
    def load_documents(self, input_path: str) -> List[Document]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–ª–∏ JSON —Ñ–∞–π–ª–∞.
        
        Args:
            input_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏–ª–∏ JSON —Ñ–∞–π–ª—É
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑: {input_path}")
        
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"–ü—É—Ç—å {input_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        documents = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø—É—Ç—å JSON —Ñ–∞–π–ª–æ–º
        if input_path.endswith('.json'):
            documents = self._load_documents_from_json(input_path)
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            documents = self._load_documents_from_directory(input_path)
        
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        return documents
    
    def _load_documents_from_json(self, json_path: str) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞."""
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON: {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã JSON
        if isinstance(data, dict):
            # –§–æ—Ä–º–∞—Ç: {id: {content: ..., metadata: ...}}
            items = list(data.items())
            for doc_id, doc_data in tqdm(items, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", unit="–¥–æ–∫"):
                content = doc_data.get('content', '')
                metadata = doc_data.get('metadata', {})
                metadata['source'] = json_path
                metadata['doc_id'] = doc_id
                
                if content:
                    documents.append(Document(page_content=content, metadata=metadata))
        elif isinstance(data, list):
            # –§–æ—Ä–º–∞—Ç: [{content: ..., metadata: ...}, ...]
            for i, doc_data in enumerate(tqdm(data, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", unit="–¥–æ–∫")):
                content = doc_data.get('content', '')
                metadata = doc_data.get('metadata', {})
                metadata['source'] = json_path
                metadata['doc_id'] = i
                
                if content:
                    documents.append(Document(page_content=content, metadata=metadata))
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON")
        return documents
    
    def _load_documents_from_directory(self, input_path: str) -> List[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        documents = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        file_types = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.md': TextLoader
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ç–∏–ø–∞–º
        for file_type, loader_class in file_types.items():
            try:
                loader = DirectoryLoader(
                    input_path,
                    glob=f"**/*{file_type}",
                    loader_cls=loader_class,
                    show_progress=True
                )
                docs = loader.load()
                documents.extend(docs)
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} —Ñ–∞–π–ª–æ–≤ —Ç–∏–ø–∞ {file_type}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–æ–≤ {file_type}: {e}")
        
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏.
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            
        Returns:
            List[Document]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        if not self.text_splitter:
            self._create_text_splitter()
        
        logger.info("–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —á–∞–Ω–∫–∏...")
        chunks = self.text_splitter.split_documents(documents)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–∞–∑–º–µ—Ä–∞–º —á–∞–Ω–∫–æ–≤
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        logger.info(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ - –º–∏–Ω: {min(chunk_sizes)}, –º–∞–∫—Å: {max(chunk_sizes)}, —Å—Ä–µ–¥–Ω–∏–π: {sum(chunk_sizes)/len(chunk_sizes):.1f}")
        
        return chunks
    
    def create_vector_store(self, chunks: List[Document], embedding_model: HuggingFaceEmbeddings) -> Union[Chroma, FAISS]:
        """
        –°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ —á–∞–Ω–∫–æ–≤.
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            Vector store –¥–ª—è –ø–æ–∏—Å–∫–∞
        """
        vector_store_config = self.config['vector_store']
        store_type = vector_store_config['type']
        persist_directory = vector_store_config['persist_directory']
        collection_name = vector_store_config.get('collection_name', 'documents')
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Ç–∏–ø–∞: {store_type}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        os.makedirs(persist_directory, exist_ok=True)
        
        try:
            if store_type == 'chroma':
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è ChromaDB
                chroma_settings = Settings(
                    persist_directory=persist_directory,
                    anonymized_telemetry=False
                )
                
                print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã ChromaDB –∏–∑ {len(chunks)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=embedding_model,
                    persist_directory=persist_directory,
                    collection_name=collection_name,
                    client_settings=chroma_settings
                )
                
            elif store_type == 'faiss':
                print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã FAISS –∏–∑ {len(chunks)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                vector_store = FAISS.from_documents(
                    documents=chunks,
                    embedding=embedding_model
                )
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å
                print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
                vector_store.save_local(persist_directory)
                
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {store_type}")
            
            self.vector_store = vector_store
            logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {persist_directory}")
            return vector_store
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
            raise
    
    def load_vector_store(self, embedding_model: HuggingFaceEmbeddings) -> Union[Chroma, FAISS]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            embedding_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            Vector store –¥–ª—è –ø–æ–∏—Å–∫–∞
        """
        vector_store_config = self.config['vector_store']
        store_type = vector_store_config['type']
        persist_directory = vector_store_config['persist_directory']
        collection_name = vector_store_config.get('collection_name', 'documents')
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã —Ç–∏–ø–∞: {store_type}")
        
        try:
            if store_type == 'chroma':
                vector_store = Chroma(
                    persist_directory=persist_directory,
                    embedding_function=embedding_model,
                    collection_name=collection_name
                )
                
            elif store_type == 'faiss':
                vector_store = FAISS.load_local(
                    persist_directory,
                    embedding_model,
                    allow_dangerous_deserialization=True
                )
                
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {store_type}")
            
            self.vector_store = vector_store
            logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return vector_store
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {e}")
            raise
    
    def create_retriever(self, search_type: str = "similarity") -> BaseRetriever:
        """
        –°–æ–∑–¥–∞–µ—Ç —Ä–µ—Ç—Ä–∏–≤–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            search_type: –¢–∏–ø –ø–æ–∏—Å–∫–∞ (similarity, mmr)
            
        Returns:
            Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.vector_store:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        retriever_config = self.config['retriever']
        k = retriever_config['k']
        
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: {search_type}, k={k}")
        
        if search_type == "similarity":
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
        elif search_type == "mmr":
            # MMR –ø–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –∏–º–ø–æ—Ä—Ç–æ–º
            logger.warning("MMR –ø–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è similarity –ø–æ–∏—Å–∫")
            retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": k}
            )
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –ø–æ–∏—Å–∫–∞: {search_type}")
        
        self.retriever = retriever
        logger.info("–†–µ—Ç—Ä–∏–≤–µ—Ä —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
        return retriever
    
    def load_qa_dataset(self, dataset_path: str) -> List[Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏.
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º
            
        Returns:
            List[Dict[str, Any]]: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤
        """
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {dataset_path}")
        
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                if dataset_path.endswith('.json'):
                    data = json.load(f)
                else:
                    raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ JSON —Ñ–∞–π–ª—ã")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
            if isinstance(data, list):
                qa_pairs = data
            elif isinstance(data, dict) and 'data' in data:
                qa_pairs = data['data']
            else:
                raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞")
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(qa_pairs)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
            return qa_pairs
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def create_sample_dataset(self, output_path: str, num_samples: int = 100) -> None:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        """
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å {num_samples} –æ–±—Ä–∞–∑—Ü–∞–º–∏")
        
        # –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        sample_qa = [
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
                "answer": "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ —Ä–∞–∑–¥–µ–ª –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∏–∑—É—á–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á –±–µ–∑ —è–≤–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π."
            },
            {
                "question": "–ö–∞–∫–∏–µ —Ç–∏–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—Ç?",
                "answer": "–°—É—â–µ—Å—Ç–≤—É—é—Ç —Ç—Ä–∏ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–∏–ø–∞ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (supervised learning), –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (unsupervised learning) –∏ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (reinforcement learning)."
            },
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏?",
                "answer": "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ ‚Äî —ç—Ç–æ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤ (–Ω–µ–π—Ä–æ–Ω–æ–≤) –∏ –º–æ–≥—É—Ç –æ–±—É—á–∞—Ç—å—Å—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏."
            },
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ?",
                "answer": "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–ª–æ—è–º–∏ (–≥–ª—É–±–æ–∫–∏–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏)."
            },
            {
                "question": "–ö–∞–∫–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏?",
                "answer": "–í –º–∞—à–∏–Ω–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã: –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π, —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å, –º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, k-–±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è k-means –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ."
            }
        ]
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–æ –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        extended_qa = []
        for i in range(num_samples):
            extended_qa.append(sample_qa[i % len(sample_qa)])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(extended_qa, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–ü—Ä–∏–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")


def create_data_processor(config: Dict[str, Any]) -> DataProcessor:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ YAML —Ñ–∞–π–ª–∞
        
    Returns:
        DataProcessor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö
    """
    return DataProcessor(config)

#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π —Ç–µ—Å—Ç–µ—Ä RAG —Å–∏—Å—Ç–µ–º—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º SberQuAD
"""

import os
import sys
import yaml
import logging
import argparse
import time
from pathlib import Path
from typing import Dict, Any, List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.pipeline import create_rag_pipeline
from src.dataset_loader import create_dataset_loader
from src.evaluation import RAGEvaluator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_rag_system(config_path: str, max_samples: int = None, rebuild_vector_db: bool = False):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç RAG —Å–∏—Å—Ç–µ–º—É –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ SberQuAD"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RAG —Å–∏—Å—Ç–µ–º—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º SberQuAD")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    dataset_loader = create_dataset_loader(config)
    evaluator = RAGEvaluator(config)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
    datasets_config = config.get('datasets', {})
    if not datasets_config:
        logger.error("–í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
        return
    
    dataset_name = list(datasets_config.keys())[0]
    dataset_config = datasets_config[dataset_name]
    dataset_path = dataset_config['path']
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–∞—Ç–∞—Å–µ—Ç
    if not os.path.exists(dataset_path):
        logger.error(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
        logger.info("–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞: python load_sberquad.py")
        return
    
    dataset_info = dataset_loader.get_dataset_info(dataset_path)
    logger.info(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ: {dataset_info}")
    
    if dataset_info['documents_count'] == 0:
        logger.error("–í –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    vector_db_path = dataset_config['vector_db_path']
    
    if rebuild_vector_db or not os.path.exists(vector_db_path):
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        dataset_loader.create_vector_store_from_dataset(dataset_path, vector_db_path)
    else:
        logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ")
    
    # –°–æ–∑–¥–∞–µ–º RAG –ø–∞–π–ø–ª–∞–π–Ω
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞...")
    pipeline = create_rag_pipeline(config)
    pipeline.initialize()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î...")
    vector_store = dataset_loader.load_vector_store_from_dataset(dataset_path, vector_db_path)
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ—Ç—Ä–∏–≤–µ—Ä —á–µ—Ä–µ–∑ DataProcessor –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    from src.data_processing import DataProcessor
    data_processor = DataProcessor(config)
    data_processor.vector_store = vector_store
    retriever = data_processor.create_retriever(config['retriever']['search_type'])
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞–π–ø–ª–∞–π–Ω —Å –Ω–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
    pipeline.vector_store = vector_store
    pipeline.retriever = retriever
    pipeline._create_qa_chain()
    
    logger.info("RAG –ø–∞–π–ø–ª–∞–π–Ω –≥–æ—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    qa_pairs_path = os.path.join(dataset_path, dataset_config['qa_pairs_file'])
    if os.path.exists(qa_pairs_path):
        import json
        with open(qa_pairs_path, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(qa_pairs)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    else:
        logger.error("–§–∞–π–ª —Å –ø–∞—Ä–∞–º–∏ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if max_samples and max_samples < len(qa_pairs):
        qa_pairs = qa_pairs[:max_samples]
        logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ –¥–æ {max_samples} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    logger.info(f"–ù–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ {len(qa_pairs)} –ø—Ä–∏–º–µ—Ä–∞—Ö")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –Ω–æ–≤—ã–º –æ—Ü–µ–Ω—â–∏–∫–æ–º
    results = evaluator.evaluate_pipeline(pipeline, qa_pairs)
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø RAG –°–ò–°–¢–ï–ú–´")
    print("="*60)
    print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {results['total_samples']}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {results['evaluation_time']:.2f} —Å–µ–∫")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
    response_times = [pred['metrics'].get('response_time', 0) for pred in results['predictions']]
    avg_response_time = sum(response_times) / len(response_times) if response_times else 0
    print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {avg_response_time:.3f} —Å–µ–∫")
    
    print("\n–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
    for metric, value in results['metrics'].items():
        if isinstance(value, float):
            print(f"  {metric}: {value:.3f}")
        else:
            print(f"  {metric}: {value}")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –≤ MLflow
    try:
        import mlflow
        import mlflow.sklearn
        
        mlflow_config = config.get('mlflow', {})
        experiment_name = mlflow_config.get('experiment_name', 'RAG_SberQuAD_Testing')
        
        mlflow.set_experiment(experiment_name)
        
        with mlflow.start_run():
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            mlflow.log_params({
                'total_queries': results['total_samples'],
                'dataset_name': dataset_name,
                'config_path': config_path,
                'max_samples': max_samples or results['total_samples']
            })
            
            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            for metric, value in results['metrics'].items():
                if isinstance(value, (int, float)):
                    mlflow.log_metric(metric, value)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
            if os.path.exists("results/rag_test_results.json"):
                mlflow.log_artifact("results/rag_test_results.json")
        
        logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ MLflow: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    os.makedirs("results", exist_ok=True)
    import json
    import numpy as np
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ numpy —Ç–∏–ø–æ–≤
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    results_converted = convert_numpy_types(results)
    results_file = "results/rag_test_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_converted, f, ensure_ascii=False, indent=2)
    
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_file}")
    logger.info("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    return results


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è RAG"""
    parser = argparse.ArgumentParser(description='RAG System Testing with SberQuAD')
    parser.add_argument('--config', type=str, default='config/base_config.yaml',
                       help='–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--rebuild-vector-db', action='store_true',
                       help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞')
    
    args = parser.parse_args()
    
    try:
        results = test_rag_system(args.config, args.max_samples, args.rebuild_vector_db)
        
        if results:
            print(f"\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ results/rag_test_results.json")
            print(f"üìà –ü—Ä–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MLflow UI: mlflow ui --backend-store-uri file:./mlruns")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        raise


if __name__ == "__main__":
    main()






